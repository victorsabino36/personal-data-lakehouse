name: CI/CD - Teste e Deploy para Cloud Composer

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:

env:
  PROJECT_ID: personal-data-lakehouse
  DBT_DIR: ./dbt/lakehouse_models
  ARTIFACTS_BUCKET: gs://personal-data-lakehouse-artifacts
  COMPOSER_DAGS_BUCKET: "gs://us-central1-personal-data-l-fed427c8-bucket/dags"

jobs:
  # ==========================================================
  # TRABALHO 1: CI (Teste de Qualidade)
  # ==========================================================
  test:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request' || github.event_name == 'workflow_dispatch' || github.event_name == 'push'
    permissions:
      contents: read
      id-token: write 
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Authenticate to Google Cloud
        uses: 'google-github-actions/auth@v2'
        with:
          workload_identity_provider: projects/${{ secrets.GCP_PROJECT_NUMBER }}/locations/global/workloadIdentityPools/github-pool/providers/github-provider
          service_account: github-actions-sa@${{ env.PROJECT_ID }}.iam.gserviceaccount.com

      # MPORTANTE: setup-gcloud ANTES de usar gcloud/gsutil
      - name: Set up Cloud SDK (gcloud CLI)
        uses: 'google-github-actions/setup-gcloud@v2'

      - name: Set up Python Environment
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: Install dbt Dependencies
        run: pip install dbt-bigquery
        
      - name: Setup dbt Profile (para testes)
        run: |
          mkdir -p ~/.dbt
          cat > ~/.dbt/profiles.yml << EOF
          lakehouse_models:
            target: github_ci
            outputs:
              github_ci:
                type: bigquery
                method: oauth
                dataset: personal_lake
                project: ${{ env.PROJECT_ID }}
                threads: 4
                location: US
          EOF
          
      - name: Run dbt build (Testa tudo)
        run: |
          cd ${{ env.DBT_DIR }}
          dbt build --target dev

  # ==========================================================
  # TRABALHO 2: CD (Deploy)
  # ==========================================================
  deploy:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'pull_request' || github.event_name == 'workflow_dispatch' || github.event_name == 'push'
    permissions:
      contents: read
      id-token: write 
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Authenticate to Google Cloud
        uses: 'google-github-actions/auth@v2'
        with:
          workload_identity_provider: projects/${{ secrets.GCP_PROJECT_NUMBER }}/locations/global/workloadIdentityPools/github-pool/providers/github-provider
          service_account: github-actions-sa@${{ env.PROJECT_ID }}.iam.gserviceaccount.com

      # CRÃTICO: Este step DEVE vir logo apÃ³s a autenticaÃ§Ã£o
      - name: Set up Cloud SDK (gcloud CLI)
        uses: 'google-github-actions/setup-gcloud@v2'

      #  Opcional mas recomendado: Verificar autenticaÃ§Ã£o
      - name: Verify Authentication
        run: |
          echo "ðŸ” Verificando autenticaÃ§Ã£o..."
          gcloud auth list
          echo " Testando acesso ao GCS..."
          gsutil ls ${{ env.COMPOSER_DAGS_BUCKET }} || echo "âŒ Falha ao acessar bucket"

      - name: Deploy DAGs para Cloud Composer
        run: |
          echo " Copiando DAGs para o Composer..."
          gsutil -m rsync -r -d airflow_home/dags/ ${{ env.COMPOSER_DAGS_BUCKET }}
          
      - name: Deploy Scripts PySpark para GCS
        run: |
          echo " Copiando scripts PySpark para Artifacts..."
          gsutil -m rsync -r -d \
            -x '.*__pycache__.*|.*\.pyc$|.*/venv/.*' \
            pipelines/ ${{ env.ARTIFACTS_BUCKET }}/pipelines
          
      - name: Deploy Projeto dbt para GCS
        run: |
          echo " Copiando projeto dbt para o bucket de DAGs do Composer..."
          gsutil -m rsync -r -d \
            -x '.*__pycache__.*|.*\.pyc$|.*/venv/.*|.*/logs/.*|.*/target/.*' \
            dbt/ ${{ env.COMPOSER_DAGS_BUCKET }}/dbt