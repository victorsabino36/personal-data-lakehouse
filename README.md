üìä Personal Data Lakehouse: Pipeline ELT de A√ß√µes FinanceirasReposit√≥rio contendo o projeto de Engenharia de Dados que implementa um Data Lakehouse completo na Google Cloud Platform (GCP). O objetivo √© demonstrar profici√™ncia em arquitetura ELT (Extract, Load, Transform), modelagem dbt e automa√ß√£o de pipelines usando GitHub Actions (Infraestrutura como C√≥digo - IaC).üí° Objetivo do ProjetoO objetivo principal √© simular um pipeline de dados financeiros para consumo, processando dados di√°rios de a√ß√µes de mercado (tickers: IBM, MSFT, NVDA). O foco √© na qualidade do c√≥digo, incrementalidade e automa√ß√£o robusta, demonstrando o ciclo de vida completo do dado.üèóÔ∏è Arquitetura e Estrutura do Reposit√≥rioO projeto segue a arquitetura Lakehouse, separando as responsabilidades de ingest√£o, transforma√ß√£o e orquestra√ß√£o.personal-data-lakehouse/
‚îú‚îÄ‚îÄ .github/                   # AUTOMA√á√ÉO & DEVOPS
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îî‚îÄ‚îÄ data_pipeline.yml  # Pipeline ELT agendado via GitHub Actions
‚îÇ
‚îú‚îÄ‚îÄ dbt/                       # TRANSFORMA√á√ÉO (T) & MODELAGEM
‚îÇ   ‚îú‚îÄ‚îÄ lakehouse_models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze/        # Defini√ß√£o da Fonte Externa (Source)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver/        # Dados limpos e padronizados
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gold/          # Agrega√ß√µes para consumo (Data Mart)
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ pipelines/                 # EXTRA√á√ÉO E CARGA (E & L)
‚îÇ   ‚îî‚îÄ‚îÄ ingest_stock_api/
‚îÇ       ‚îî‚îÄ‚îÄ ingest_stocks.py   # Script de ingest√£o da API Alpha Vantage
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md
üõ†Ô∏è Tecnologias UtilizadasCategoriaFerramentaProp√≥sito no PipelineInfraestrutura CloudGoogle Cloud Platform (GCP)Hospedagem principal.Data WarehouseBigQueryPlataforma de processamento SQL e destino final dos dados.Modelagem/ELTdbt (Data Build Tool)Materializa√ß√£o incremental, testes de qualidade e modelagem Silver/Gold.Orquestra√ß√£o (IaC)GitHub ActionsAgendamento di√°rio e execu√ß√£o automatizada do workflow Python ‚Üí dbt.Ingest√£o/Extra√ß√£oPython (requests, pandas)Busca de dados da API e carregamento inicial em Parquet/GCS.Formato de DadosParquetArmazenamento colunar eficiente no Data Lake (GCS).üíß Camadas do Data Lakehouse (BigQuery)O pipeline garante a qualidade e a performance segregando os dados em camadas:CamadaDataset de DestinoProp√≥sitoMaterializa√ß√£oBronzestock_bronzeDados brutos, sem tratamento, mas formatados (Parquet no GCS).TRUNCATE (Atualizado diariamente)Silverstock_silverDados limpos, com tipos e timestamps corrigidos. Base para transforma√ß√µes.IncrementalGoldstock_goldAgrega√ß√µes de neg√≥cio (Ex: Resumo Mensal). Otimizado para BI e An√°lise.IncrementalüöÄ Configura√ß√£o e Automa√ß√£oPr√©-requisitosPython 3.11+dbt-bigquery (pip install dbt-bigquery)Conta GCP.1. Autentica√ß√£o na NuvemO pipeline usa o m√©todo seguro Workload Identity Federation para autenticar o GitHub Actions no GCP.Crie uma Service Account (ex: github-actions-sa) com pap√©is BigQuery Data Editor, Storage Admin, e Service Account Token Creator.Configure os Repository Secrets no GitHub:ALPHA_VANTAGE_API_KEYGCP_PROJECT_NUMBER (O n√∫mero do seu projeto GCP).2. Execu√ß√£o Local (Para Desenvolvimento)Para testar o pipeline completo localmente (simulando o Actions):# 1. Instala depend√™ncias
pip install -r ./pipelines/requirements.txt -r ./dbt/requirements.txt

# 2. Testa a ingest√£o (Extrai API -> Carrega Bronze)
python ./pipelines/ingest_stock_api/ingest_stocks.py

# 3. Testa a transforma√ß√£o (Bronze -> Silver -> Gold)
cd dbt/lakehouse_models
dbt run --full-refresh # Roda tudo do zero para valida√ß√£o
dbt test # Executa os testes de qualidade
ü§ù Contribui√ß√£oSiga o fluxo padr√£o de desenvolvimento:Crie um branch para o recurso: git checkout -b feature/nova-featureFa√ßa o commit das altera√ß√µes e push.Abra um Pull Request para o branch main.