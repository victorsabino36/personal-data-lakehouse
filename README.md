# üìä Personal Data Lakehouse: 

Reposit√≥rio contendo o projeto de **Engenharia de Dados** que implementa um *Data Lakehouse* completo na **Google Cloud Platform (GCP)**. O objetivo √© demonstrar profici√™ncia em arquitetura **ETL** (Extract,  Transform, Load), modelagem **dbt** e automa√ß√£o de *pipelines* usando **GitHub Actions** (Infraestrutura como C√≥digo - IaC).

---

## üí° Objetivo do Projeto

O objetivo principal √© simular um *pipeline* de dados financeiros para consumo, processando dados di√°rios de a√ß√µes de mercado (tickers: IBM, MSFT, NVDA). O foco √© na **qualidade do c√≥digo**, **incrementalidade** e **automa√ß√£o robusta**, demonstrando o ciclo de vida completo do dado.

## üèóÔ∏è Arquitetura e Estrutura do Reposit√≥rio

O projeto segue a arquitetura **Lakehouse**, separando as responsabilidades de ingest√£o, transforma√ß√£o e orquestra√ß√£o.

```tree
personal-data-lakehouse/
‚îú‚îÄ‚îÄ .github/                   # CONFIGURA√á√ÉO DE CI/CD
‚îÇ   ‚îî‚îÄ‚îÄ workflows/             # (Cont√©m data_pipeline.yml)
‚îú‚îÄ‚îÄ dags/                      # ORQUESTRA√á√ÉO (Estrutura para Airflow/Composer)
‚îú‚îÄ‚îÄ dbt/                       # TRANSFORMA√á√ÉO & MODELAGEM (ELT - T)
‚îÇ   ‚îî‚îÄ‚îÄ lakehouse_models/
‚îÇ       ‚îú‚îÄ‚îÄ models/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ bronze/        # Defini√ß√£o das fontes externas (sources)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ silver/        # Modelos limpos e padronizados
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ gold/          # Modelos agregados para BI (Data Marts)
‚îÇ       ‚îî‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ pipelines/                 # C√ìDIGO DE INGEST√ÉO (ETL)
‚îÇ   
‚îú‚îÄ‚îÄ .gitignore
‚îî‚îÄ‚îÄ README.md                  # Documenta√ß√£o principal
```

## üõ†Ô∏è Tecnologias Utilizadas

| Categoria | Ferramenta | Prop√≥sito no Pipeline |
| :--- | :--- | :--- |
| **Infraestrutura Cloud** | **Google Cloud Platform (GCP)** | Hospedagem principal. |
| **Data Warehouse** | **BigQuery** | Plataforma de processamento SQL e destino final dos dados. |
| **Modelagem/ELT** | **dbt (Data Build Tool)** | Materializa√ß√£o incremental, testes de qualidade e modelagem Silver/Gold. |
| **Orquestra√ß√£o (IaC)** | **GitHub Actions** | Agendamento di√°rio e execu√ß√£o automatizada do *workflow* Python ‚Üí dbt. |
| **Ingest√£o/Extra√ß√£o**| **Python** (`requests`, `pandas`) | Busca de dados da API e carregamento inicial em Parquet/GCS. |
| **Formato de Dados** | **Parquet** | Armazenamento colunar eficiente no Data Lake (GCS). |

---

## üíß Camadas do Data Lakehouse (BigQuery)

O pipeline garante a qualidade e a performance segregando os dados em camadas:

| Camada | Dataset de Destino | Prop√≥sito | Materializa√ß√£o |
| :--- | :--- | :--- | :--- |
| **Bronze** | `{}_bronze` | Dados brutos, sem tratamento, mas formatados (Parquet no GCS). 
| **Silver** | `{}_silver` | Dados limpos, com tipos e *timestamps* corrigidos. Base para transforma√ß√µes. 
| **Gold** | `{}_gold` | Agrega√ß√µes de neg√≥cio (Ex: Resumo Mensal). Otimizado para BI e An√°lise. 

---

## üöÄ Configura√ß√£o e Automa√ß√£o

### Pr√©-requisitos

* Python 3.11+
* dbt-bigquery (`pip install dbt-bigquery`)
* Conta GCP.

### 1. Autentica√ß√£o na Nuvem

O pipeline usa o m√©todo seguro **Workload Identity Federation** para autenticar o GitHub Actions no GCP.

1.  Crie uma Service Account (ex: `github-actions-sa`) com pap√©is `BigQuery Data Editor`, `Storage Admin`, e `Service Account Token Creator`.
2.  Configure os seguintes **Repository Secrets** no GitHub:
    * `ALPHA_VANTAGE_API_KEY`
    * `GCP_PROJECT_NUMBER`

### 2. Execu√ß√£o Local (Para Desenvolvimento)

Para testar o pipeline completo localmente (simulando o Actions):

```bash
# 1. Instala depend√™ncias
pip install -r ./pipelines/requirements.txt -r ./dbt/requirements.txt

# 2. Testa a ingest√£o (Extrai API -> Carrega Bronze)
python ./pipelines/ingest_stock_api/ingest_stocks.py

# 3. Testa a transforma√ß√£o (Bronze -> Silver -> Gold)
cd dbt/lakehouse_models
dbt run --full-refresh # Roda tudo do zero para valida√ß√£o
dbt test # Executa os testes de qualidade


