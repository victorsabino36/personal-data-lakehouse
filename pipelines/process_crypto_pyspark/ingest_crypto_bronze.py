"""
ETL Bronze Layer - Criptomoedas
SoluÃ§Ã£o SEM JARS: Salva local em Delta Lake e depois faz upload para GCS
"""
import os
import sys
import requests
import json
from datetime import datetime
from google.cloud import storage

# ForÃ§a o Spark a usar o Python correto
os.environ['PYSPARK_PYTHON'] = sys.executable
os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    current_timestamp, lit, col, 
    to_date, year, month, dayofmonth
)
from delta import configure_spark_with_delta_pip

# ============================================================================
# CONFIGURAÃ‡Ã•ES
# ============================================================================
PROJECT_ID = "personal-data-lakehouse"
GCS_BUCKET_NAME = "date_lakehouse_bronze"

# Paths LOCAIS (sem gs://)
LOCAL_BRONZE_PATH = "data/bronze/crypto_markets"
GCS_BRONZE_PREFIX = "bronze-crypto/crypto_markets"  # Prefixo no bucket

# API CoinGecko - Top 10 criptos
API_URL = (
    "https://api.coingecko.com/api/v3/coins/markets"
    "?vs_currency=brl"
    "&ids=bitcoin,ethereum,solana,cardano,ripple,polkadot,dogecoin,avalanche-2,chainlink,matic-network"
    "&order=market_cap_desc"
    "&per_page=10"
    "&sparkline=false"
)


# ============================================================================
# FUNÃ‡Ã•ES AUXILIARES
# ============================================================================
def create_spark_session() -> SparkSession:
    """Cria sessÃ£o Spark APENAS para Delta Lake LOCAL"""
    print("ğŸš€ Iniciando Spark Session (modo local)...")
    
    builder = (
        SparkSession.builder
        .appName("CryptoIngestionLocal")
        .master("local[*]")
        
        # Delta Lake Extensions
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        
        # OtimizaÃ§Ãµes
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.driver.host", "127.0.0.1")
        .config("spark.driver.memory", "2g")
    )
    
    spark = configure_spark_with_delta_pip(builder).getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    
    print(f"âœ… Spark {spark.version} iniciado com sucesso!")
    return spark


def fetch_crypto_data(url: str, retries: int = 3) -> list:
    """Busca dados da API CoinGecko"""
    print(f"\nğŸ“¡ Buscando dados da API CoinGecko...")
    
    for attempt in range(1, retries + 1):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if not data or not isinstance(data, list):
                print(f"âš ï¸  API retornou dados invÃ¡lidos")
                return None
            
            print(f"âœ… {len(data)} criptomoedas coletadas")
            return data
            
        except Exception as e:
            print(f"âŒ Tentativa {attempt}/{retries} falhou: {e}")
            if attempt < retries:
                import time
                time.sleep(5)
    
    return None


def save_to_local_delta(df, path: str, mode: str = "append"):
    """Salva DataFrame como Delta Table LOCALMENTE"""
    print(f"\nğŸ’¾ Salvando dados localmente...")
    print(f"   Destino: {path}")
    print(f"   Modo: {mode}")
    print(f"   Registros: {df.count()}")
    
    try:
        # Adiciona colunas de particionamento
        df_partitioned = (
            df
            .withColumn("ingestion_date", to_date(col("data_ingestao")))
            .withColumn("year", year(col("ingestion_date")))
            .withColumn("month", month(col("ingestion_date")))
        )
        
        # Salva localmente
        (
            df_partitioned.write
            .format("delta")
            .mode(mode)
            .partitionBy("year", "month")
            .option("overwriteSchema", "true")
            .save(path)
        )
        
        print("âœ… Dados salvos localmente com sucesso!")
        return True
        
    except Exception as e:
        print(f"âŒ Erro ao salvar localmente: {e}")
        return False


def upload_to_gcs(local_path: str, bucket_name: str, gcs_prefix: str):
    """
    Faz upload da Delta Table local para o GCS
    
    Args:
        local_path: Caminho local da Delta Table
        bucket_name: Nome do bucket GCS
        gcs_prefix: Prefixo no bucket (ex: 'bronze/crypto')
    """
    print(f"\nâ˜ï¸  Fazendo upload para GCS...")
    print(f"   Bucket: {bucket_name}")
    print(f"   Prefixo: {gcs_prefix}")
    
    try:
        # Inicializa cliente GCS
        client = storage.Client(project=PROJECT_ID)
        bucket = client.bucket(bucket_name)
        
        # Conta arquivos
        total_files = 0
        uploaded_files = 0
        
        # Percorre todos os arquivos locais
        for root, dirs, files in os.walk(local_path):
            for file in files:
                total_files += 1
                local_file = os.path.join(root, file)
                
                # Calcula caminho relativo
                relative_path = os.path.relpath(local_file, local_path)
                gcs_path = f"{gcs_prefix}/{relative_path}"
                
                # Upload
                blob = bucket.blob(gcs_path)
                blob.upload_from_filename(local_file)
                uploaded_files += 1
                
                if uploaded_files % 10 == 0:
                    print(f"   ğŸ“¤ {uploaded_files}/{total_files} arquivos...")
        
        print(f"âœ… Upload concluÃ­do: {uploaded_files} arquivos enviados!")
        print(f"   URL: gs://{bucket_name}/{gcs_prefix}/")
        return True
        
    except Exception as e:
        print(f"âŒ Erro no upload para GCS: {e}")
        print(f"   Verifique se vocÃª estÃ¡ autenticado:")
        print(f"   gcloud auth application-default login")
        return False


def read_local_delta(spark, path: str):
    """LÃª e mostra estatÃ­sticas da Delta Table local"""
    try:
        df = spark.read.format("delta").load(path)
        total = df.count()
        
        print(f"\nğŸ“Š EstatÃ­sticas da Tabela Local:")
        print(f"   Total de registros: {total}")
        print(f"   Criptos Ãºnicas: {df.select('id').distinct().count()}")
        
        # Top 5
        print(f"\nğŸ† Top 5 Criptomoedas:")
        (
            df.select("symbol", "name", "current_price", "market_cap_rank")
            .orderBy("market_cap_rank")
            .show(5, truncate=False)
        )
        
    except Exception as e:
        print(f"âš ï¸  NÃ£o foi possÃ­vel ler tabela local: {e}")


# ============================================================================
# PIPELINE PRINCIPAL
# ============================================================================
def main():
    """Executa o pipeline completo"""
    
    print("\n" + "="*80)
    print("ğŸš€ ETL BRONZE LAYER - CRIPTOMOEDAS (LOCAL + GCS)")
    print("="*80)
    print(f"ğŸ“… Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ’¾ Destino Local: {LOCAL_BRONZE_PATH}")
    print(f"â˜ï¸  Destino GCS: gs://{GCS_BUCKET_NAME}/{GCS_BRONZE_PREFIX}/")
    
    spark = None
    
    try:
        # 1. Cria Spark Session
        spark = create_spark_session()
        
        # 2. Busca dados da API
        json_data = fetch_crypto_data(API_URL)
        
        if not json_data:
            print("\nâŒ Falha ao obter dados da API")
            return
        
        # 3. Converte para DataFrame Spark
        print(f"\nğŸ”„ Convertendo {len(json_data)} registros para DataFrame...")
        json_strings = [json.dumps(record) for record in json_data]
        rdd = spark.sparkContext.parallelize(json_strings)
        df = spark.read.json(rdd)
        
        # Adiciona timestamp
        df = df.withColumn("data_ingestao", current_timestamp())
        
        print("âœ… DataFrame criado!")
        
        # 4. Salva LOCALMENTE em Delta Lake
        success = save_to_local_delta(df, LOCAL_BRONZE_PATH, mode="append")
        
        if not success:
            print("\nâŒ Falha ao salvar localmente")
            return
        
        # 5. Mostra dados locais
        read_local_delta(spark, LOCAL_BRONZE_PATH)
        
        # 6. Upload para GCS
        print("\n" + "="*80)
        upload_success = upload_to_gcs(
            LOCAL_BRONZE_PATH, 
            GCS_BUCKET_NAME, 
            GCS_BRONZE_PREFIX
        )
        
        if upload_success:
            print("\n" + "="*80)
            print("âœ… ETL CONCLUÃDO COM SUCESSO!")
            print("="*80)
            print("\nğŸ“ Seus dados estÃ£o em:")
            print(f"   1. Local: {LOCAL_BRONZE_PATH}")
            print(f"   2. GCS: gs://{GCS_BUCKET_NAME}/{GCS_BRONZE_PREFIX}/")
            print("\nğŸ’¡ Para ler do GCS, use:")
            print(f"   gsutil ls gs://{GCS_BUCKET_NAME}/{GCS_BRONZE_PREFIX}/")
        else:
            print("\nâš ï¸  ETL executado, mas upload para GCS falhou")
            print(f"   Dados salvos localmente em: {LOCAL_BRONZE_PATH}")
        
    except Exception as e:
        print(f"\nâŒ ERRO FATAL: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        if spark:
            print("\nğŸ›‘ Encerrando Spark...")
            spark.stop()


if __name__ == "__main__":
    # Cria diretÃ³rio local se nÃ£o existir
    os.makedirs(os.path.dirname(LOCAL_BRONZE_PATH), exist_ok=True)
    main()