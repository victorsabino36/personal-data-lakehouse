import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, current_timestamp, round

# ============================================================================
# CONFIGURA√á√ïES DA CAMADA SILVER
# ============================================================================
GCS_BUCKET_NAME = "date_lakehouse_bronze" 
GCS_BRONZE_PATH = f"gs://{GCS_BUCKET_NAME}/bronze-crypto/crypto_markets"
GCS_SILVER_PATH = f"gs://{GCS_BUCKET_NAME}/silver-crypto/crypto_refined" 

# Configura√ß√µes BigQuery
BQ_PROJECT = "personal-data-lakehouse"
BQ_DATASET = "silver_crypto"
BQ_TABLE = "crypto_favorites"
BQ_FULL_TABLE = f"{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}"
TEMP_BUCKET = "dataproc-staging-us-central1-1015215278127-eh5ygvtr"

# ============================================================================
# FUN√á√ïES DA CAMADA SILVER
# ============================================================================

def create_spark_session() -> SparkSession:
    """Cria a sess√£o Spark com Delta configurado"""
    # (Fun√ß√£o j√° definida, mantida para contexto)
    print("üöÄ Iniciando Spark Session (Dataproc)...")
    builder = (
        SparkSession.builder
        .appName("CryptoRefinementSilver")
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        .config("spark.sql.adaptive.enabled", "true")
    )
    spark = builder.getOrCreate()
    spark.sparkContext.setLogLevel("WARN")
    print(f"‚úÖ Spark {spark.version} iniciado!")
    return spark

def read_bronze_data(spark: SparkSession, path: str):
    """L√™ os dados brutos da Delta Table (Camada Bronze)"""
    print(f"\nüìö Lendo dados da Camada Bronze: {path}")
    try:
        df = spark.read.format("delta").load(path)
        print(f"‚úÖ Dados lidos. Total de registros: {df.count()}")
        return df
    except Exception as e:
        print(f"‚ùå Erro ao ler a Camada Bronze: {e}")
        return None

def transform_to_silver(df_bronze):
    """Aplica limpeza, padroniza√ß√£o e refino para a Camada Silver"""
    print("\n‚öôÔ∏è  Iniciando transforma√ß√µes para a Camada Silver...")
    
    # 1. Sele√ß√£o, Renomea√ß√£o e Casting de Tipos (Limpeza)
    df_silver = (
        df_bronze
        .select(
            # Renomeia ID
            col("id").alias("crypto_id"),
            col("symbol").alias("simbolo"),
            col("name").alias("nome"),
            
            # Padroniza e Converte Tipos Num√©ricos
            col("current_price").cast("decimal(20, 4)").alias("preco_atual_brl"),
            col("market_cap").cast("bigint").alias("capitalizacao_mercado"),
            col("total_volume").cast("bigint").alias("volume_total_24h"),
            round(col("price_change_percentage_24h"), 4).alias("variacao_preco_24h_perc"),
            col("circulating_supply").cast("decimal(30, 8)").alias("oferta_circulante"),
            
            # Seleciona Metadados de Ingest√£o
            col("data_ingestao") # Coluna de timestamp RAW
        )
        # 2. Remo√ß√£o de Registros com Pre√ßo Nulo (Qualidade)
        .filter(col("preco_atual_brl").isNotNull())
        # 3. Enriquecimento (Adiciona Timestamp Silver)
        .withColumn("data_refino", current_timestamp())
    )
    
    print("‚úÖ Transforma√ß√µes da Camada Silver conclu√≠das.")
    return df_silver

def write_silver_data(df_silver, path: str, mode: str = "overwrite"):
    """Salva o DataFrame refinado como Delta Table na Camada Silver"""
    print(f"\nüíæ Salvando dados na Camada Silver: {path}")
    try:
        (
            df_silver.write
            .format("delta")
            .mode(mode)
            .option("mergeSchema", "true") # Permite adi√ß√£o segura de novas colunas
            # A camada Silver n√£o costuma usar as mesmas parti√ß√µes da Bronze
            .save(path) 
        )
        print("‚úÖ Dados salvos com sucesso na Camada Silver!")
        return True
    except Exception as e:
        print(f"‚ùå Erro ao salvar na Camada Silver: {e}")
        return False
    
def write_silver_bigquery(df_silver, table: str, temp_bucket: str, mode: str = "overwrite"):
    """Salva o DataFrame refinado no BigQuery"""
    print(f"\nüíæ Salvando dados no BigQuery: {table}")
    try:
        (
            df_silver.write
            .format("bigquery")
            .option("writeMethod", "direct")
            .option("temporaryGcsBucket", temp_bucket)
            .mode(mode)
            .save(table)
        )
        print("‚úÖ Dados salvos com sucesso no BigQuery!")
        return True
    except Exception as e:
        print(f"‚ùå Erro ao salvar no BigQuery: {e}")
        return False

# ============================================================================
# PIPELINE SILVER PRINCIPAL
# ============================================================================
def silver_main():
    print("\n" + "="*80)
    print("‚öôÔ∏è  PIPELINE SILVER LAYER - REFINO DE CRIPTOMOEDAS")
    print(f"‚òÅÔ∏è  Destino Silver: {GCS_SILVER_PATH}")
    
    spark = None
    try:
        spark = create_spark_session()
        
        # Leitura
        df_bronze = read_bronze_data(spark, GCS_BRONZE_PATH)
        if df_bronze is None:
            raise Exception("N√£o foi poss√≠vel ler os dados da Camada Bronze.")

        # Transforma√ß√£o
        df_silver = transform_to_silver(df_bronze)
        
        # Grava√ß√£o
        if not write_silver_data(df_silver, GCS_SILVER_PATH, mode="overwrite"):
            raise Exception("Falha ao salvar os dados na Camada Silver.")

        # Grava√ß√£o no BigQuery  
        if not write_silver_bigquery(df_silver, BQ_FULL_TABLE, TEMP_BUCKET, mode="overwrite"):
            raise Exception("Falha ao salvar os dados no BigQuery.")

        print("\n‚úÖ REFINO DA CAMADA SILVER CONCLU√çDO COM SUCESSO!")
        print("‚úÖ Dados dispon√≠veis em Delta Lake (GCS) e BigQuery!") 
        
    except Exception as e:
        print(f"\n‚ùå ERRO FATAL NO SILVER: {e}")
        sys.exit(1)
    finally:
        if spark:
            print("\nüõë Encerrando Spark...")
            spark.stop()

if __name__ == "__main__":
    # Para rodar este script separadamente
    silver_main()